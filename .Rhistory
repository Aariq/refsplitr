}
ceiling(check.row/max.point)
max.point<-100000
ceiling(check.row/max.point)
ceiling(nrow(dist.frame.full)/max.point)
ceiling(nrow(dist.frame.full)/max.point)
max.point<-1000000
for(i in 1:ceiling(nrow(dist.frame.full)/max.point) ){
#i<-1
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>check.row){max.row<-check.row}
#nrowD<-max.row-min.row + 1
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
gg<-dist.frame[1:100,]
# attempt to lower the amount of important points by filtering points that are far away (HARD ESTIMATE)
dist.frame<-subset(dist.frame, abs(x1-x2)<0.01 & abs(y1-y2)<0.01)
if(nrow(dist.frame)>0){
dist.frame$dist<-gdist(dist.frame$x1, dist.frame$y1,dist.frame$x2, dist.frame$y2, 'm')
dist.frame<-dist.frame[dist.frame$dist<=buffer ,]
dist.list[[paste0(i)]]<-dist.frame
}
}
rm(dist.frame)
gc()
dist.frame1<-do.call(rbind, dist.list)
##############################
# option
#############################
buffer<-40 #in meters
earlydate<-0 # earliest julian date to filter by
latedate<-365 # latest julian date to filter by
day.filter<-1 # How many points required to be taken per day to expect we catch a nesting event
day.threshold<-2 # consecutive days at location required
day.threshold.start<-1
daily.visit.threshold<-1 # amount of consecutive visits within a day required
max.visit.threshold<-1
percent.time.at.site.t<-1
resampleBuffer<-20
percent.max.visit.t<-1
year.start<-274
window_size<-120
StorkID<-"1134420-2013" #This is technically the 'burst' id.
raw.data<-read.csv('file:///C:/Users/birde/Downloads/MEGU_nesters.csv')
############################
StorkID<-unique(raw.data$burst)[5]
gc()
##############################
# option
#############################
buffer<-40 #in meters
earlydate<-0 # earliest julian date to filter by
latedate<-365 # latest julian date to filter by
day.filter<-1 # How many points required to be taken per day to expect we catch a nesting event
day.threshold<-2 # consecutive days at location required
day.threshold.start<-1
daily.visit.threshold<-1 # amount of consecutive visits within a day required
max.visit.threshold<-1
percent.time.at.site.t<-1
resampleBuffer<-20
percent.max.visit.t<-1
year.start<-274
window_size<-120
StorkID<-"1134420-2013" #This is technically the 'burst' id.
raw.data<-read.csv('file:///C:/Users/birde/Downloads/MEGU_nesters.csv')
############################
StorkID<-unique(raw.data$burst)[5]
data<-subset(raw.data,burst==StorkID)
##############################
data$x<-data$long
data$y<-data$lat
data$julian<-as.POSIXlt(data$date)$yday+1
# filter out by the maxdate (late in season), break from function if not (need to add a stop)
if(!any(data$julian<latedate)){return()}
maxday<-max(data$julian[data$julian<latedate])
# Have to calculate a 'relative date' since nesting days likely cross the new year (julian day 365 versus 0)
data$reljulian<-data$julian
data$reljulian[data$reljulian>year.start]<-data$reljulian[data$reljulian>year.start]-365
newjul<-data.frame(reljulian=min(data$reljulian):max(data$reljulian[data$reljulian<latedate]))
newjul$reldate<-1:nrow(newjul)
newjul$julian<-newjul$reljulian
newjul$julian[newjul$julian<0]<-newjul$julian[newjul$julian<0]+365
# create a sequential id
data<-data[order(data$date),]
data$seqid<-1:nrow(data)
#create unique location ids
### Create unique x,y combinations
#################################
xy<-as.factor(paste0(data$x,'_',data$y))
data$locid<-as.numeric(xy)
df1<-data.frame(xy=levels(xy))
df1$locid<-as.numeric(df1$xy)
df1$groupid<-df1$locid
df1$x<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[1]))
df1$y<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[2]))
f<-table(merge(raw.data[,c('long','lat','X')],df1[,c('x','y','locid')],by.x=c('long','lat'),by.y=c('x','y'),all.x=T)$locid)
f<-sort(f, decreasing=T)
df1<-merge(df1,data.frame(freq=c(f),locid=as.numeric(names(f))),by='locid',all.x=T)
library(Imap)
check.row<-nrow(df1)
dist.frame.full<-data.frame(first=rep(1:check.row,times=seq((check.row-1),0)),second=unlist(sapply(2:check.row,function(x) seq(x,check.row))))
dist.list<-list()
max.point<-1000000
print(i/ceiling(nrow(dist.frame.full)/max.point))
i<-1
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>check.row){max.row<-check.row}
dist.frame<-dist.frame.full[min.row:max.row,]
max.row
ceiling(nrow(dist.frame.full)/max.point)
51*10014
i*max.point
max.row<- i*max.point
max.row
if(max.row>check.row){max.row<-nrow(dist.frame.full)}
nbigrow<-nrow(dist.frame.full)
i<-1
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
gg<-dist.frame[1:100,]
# attempt to lower the amount of important points by filtering points that are far away (HARD ESTIMATE)
dist.frame<-subset(dist.frame, abs(x1-x2)<0.01 & abs(y1-y2)<0.01)
if(nrow(dist.frame)>0){
dist.frame$dist<-gdist(dist.frame$x1, dist.frame$y1,dist.frame$x2, dist.frame$y2, 'm')
dist.frame<-dist.frame[dist.frame$dist<=buffer ,]
dist.list[[paste0(i)]]<-dist.frame
}
rm(dist.frame)
gc()
i<-2
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
gg<-dist.frame[1:100,]
# attempt to lower the amount of important points by filtering points that are far away (HARD ESTIMATE)
dist.frame<-subset(dist.frame, abs(x1-x2)<0.01 & abs(y1-y2)<0.01)
if(nrow(dist.frame)>0){
dist.frame$dist<-gdist(dist.frame$x1, dist.frame$y1,dist.frame$x2, dist.frame$y2, 'm')
dist.frame<-dist.frame[dist.frame$dist<=buffer ,]
dist.list[[paste0(i)]]<-dist.frame
}
rm(dist.frame)
gc()
rm(dist.frame)
gc()
print(i/ceiling(nrow(dist.frame.full)/max.point))
print(paste0(i,'/',ceiling(nrow(dist.frame.full)/max.point)))
dist.list<-list()
max.point<-1000000
for(i in 1:ceiling(nbigrow/max.point) ){
#i<-2
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
#nrowD<-max.row-min.row + 1
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
gg<-dist.frame[1:100,]
# attempt to lower the amount of important points by filtering points that are far away (HARD ESTIMATE)
dist.frame<-subset(dist.frame, abs(x1-x2)<0.01 & abs(y1-y2)<0.01)
if(nrow(dist.frame)>0){
dist.frame$dist<-gdist(dist.frame$x1, dist.frame$y1,dist.frame$x2, dist.frame$y2, 'm')
dist.frame<-dist.frame[dist.frame$dist<=buffer ,]
dist.list[[paste0(i)]]<-dist.frame
}
rm(dist.frame)
gc()
print(paste0(i,'/',ceiling(nrow(dist.frame.full)/max.point)))
}
##############################
# option
#############################
buffer<-40 #in meters
earlydate<-0 # earliest julian date to filter by
latedate<-365 # latest julian date to filter by
day.filter<-1 # How many points required to be taken per day to expect we catch a nesting event
day.threshold<-2 # consecutive days at location required
day.threshold.start<-1
daily.visit.threshold<-1 # amount of consecutive visits within a day required
max.visit.threshold<-1
percent.time.at.site.t<-1
resampleBuffer<-20
percent.max.visit.t<-1
year.start<-274
window_size<-120
StorkID<-"1134420-2013" #This is technically the 'burst' id.
raw.data<-read.csv('file:///C:/Users/birde/Downloads/MEGU_nesters.csv')
############################
StorkID<-unique(raw.data$burst)[5]
data<-subset(raw.data,burst==StorkID)
##############################
data$x<-data$long
data$y<-data$lat
data$julian<-as.POSIXlt(data$date)$yday+1
# filter out by the maxdate (late in season), break from function if not (need to add a stop)
if(!any(data$julian<latedate)){return()}
maxday<-max(data$julian[data$julian<latedate])
# Have to calculate a 'relative date' since nesting days likely cross the new year (julian day 365 versus 0)
data$reljulian<-data$julian
data$reljulian[data$reljulian>year.start]<-data$reljulian[data$reljulian>year.start]-365
newjul<-data.frame(reljulian=min(data$reljulian):max(data$reljulian[data$reljulian<latedate]))
newjul$reldate<-1:nrow(newjul)
newjul$julian<-newjul$reljulian
newjul$julian[newjul$julian<0]<-newjul$julian[newjul$julian<0]+365
# create a sequential id
data<-data[order(data$date),]
data$seqid<-1:nrow(data)
#create unique location ids
### Create unique x,y combinations
#################################
xy<-as.factor(paste0(data$x,'_',data$y))
data$locid<-as.numeric(xy)
df1<-data.frame(xy=levels(xy))
df1$locid<-as.numeric(df1$xy)
df1$groupid<-df1$locid
df1$x<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[1]))
df1$y<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[2]))
f<-table(merge(raw.data[,c('long','lat','X')],df1[,c('x','y','locid')],by.x=c('long','lat'),by.y=c('x','y'),all.x=T)$locid)
f<-sort(f, decreasing=T)
df1<-merge(df1,data.frame(freq=c(f),locid=as.numeric(names(f))),by='locid',all.x=T)
library(Imap)
check.row<-nrow(df1)
dist.frame.full<-data.frame(first=rep(1:check.row,times=seq((check.row-1),0)),second=unlist(sapply(2:check.row,function(x) seq(x,check.row))))
nbigrow<-nrow(dist.frame.full)
dist.list<-list()
max.point<-10000000
for(i in 1:ceiling(nbigrow/max.point) ){
#i<-2
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
#nrowD<-max.row-min.row + 1
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
gg<-dist.frame[1:100,]
# attempt to lower the amount of important points by filtering points that are far away (HARD ESTIMATE)
dist.frame<-subset(dist.frame, abs(x1-x2)<0.01 & abs(y1-y2)<0.01)
if(nrow(dist.frame)>0){
dist.frame$dist<-gdist(dist.frame$x1, dist.frame$y1,dist.frame$x2, dist.frame$y2, 'm')
dist.frame<-dist.frame[dist.frame$dist<=buffer ,]
dist.list[[paste0(i)]]<-dist.frame
}
rm(dist.frame)
gc()
print(paste0(i,'/',ceiling(nrow(dist.frame.full)/max.point)))
}
##############################
# option
#############################
buffer<-40 #in meters
earlydate<-0 # earliest julian date to filter by
latedate<-365 # latest julian date to filter by
day.filter<-1 # How many points required to be taken per day to expect we catch a nesting event
day.threshold<-2 # consecutive days at location required
day.threshold.start<-1
daily.visit.threshold<-1 # amount of consecutive visits within a day required
max.visit.threshold<-1
percent.time.at.site.t<-1
resampleBuffer<-20
percent.max.visit.t<-1
year.start<-274
window_size<-120
StorkID<-"1134420-2013" #This is technically the 'burst' id.
raw.data<-read.csv('file:///C:/Users/birde/Downloads/MEGU_nesters.csv')
############################
StorkID<-unique(raw.data$burst)[5]
# This function calculates the likely
data<-subset(raw.data,burst==StorkID)
##############################
data$x<-data$long
data$y<-data$lat
data$julian<-as.POSIXlt(data$date)$yday+1
# filter out by the maxdate (late in season), break from function if not (need to add a stop)
if(!any(data$julian<latedate)){return()}
maxday<-max(data$julian[data$julian<latedate])
# Have to calculate a 'relative date' since nesting days likely cross the new year (julian day 365 versus 0)
data$reljulian<-data$julian
data$reljulian[data$reljulian>year.start]<-data$reljulian[data$reljulian>year.start]-365
newjul<-data.frame(reljulian=min(data$reljulian):max(data$reljulian[data$reljulian<latedate]))
newjul$reldate<-1:nrow(newjul)
newjul$julian<-newjul$reljulian
newjul$julian[newjul$julian<0]<-newjul$julian[newjul$julian<0]+365
# create a sequential id
data<-data[order(data$date),]
data$seqid<-1:nrow(data)
#create unique location ids
### Create unique x,y combinations
#################################
xy<-as.factor(paste0(data$x,'_',data$y))
data$locid<-as.numeric(xy)
df1<-data.frame(xy=levels(xy))
df1$locid<-as.numeric(df1$xy)
df1$groupid<-df1$locid
df1$x<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[1]))
df1$y<-sapply(strsplit(as.character(df1$xy),'_'),function(x)as.numeric(x[2]))
f<-table(merge(raw.data[,c('long','lat','X')],df1[,c('x','y','locid')],by.x=c('long','lat'),by.y=c('x','y'),all.x=T)$locid)
f<-sort(f, decreasing=T)
df1<-merge(df1,data.frame(freq=c(f),locid=as.numeric(names(f))),by='locid',all.x=T)
library(Imap)
check.row<-nrow(df1)
dist.frame.full<-data.frame(first=rep(1:check.row,times=seq((check.row-1),0)),second=unlist(sapply(2:check.row,function(x) seq(x,check.row))))
nbigrow<-nrow(dist.frame.full)
dist.list<-list()
max.point<-10000000
i<-1
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
rm(dist.frame)
gc()
max.point<-1000000
i<-1
min.row<-(i-1)*max.point + 1
max.row<- i*max.point
if(max.row>nbigrow){max.row<-nbigrow}
dist.frame<-dist.frame.full[min.row:max.row,]
dist.frame[,c('x1','y1','id1')]<-merge(dist.frame[,'first',drop=F],df1[,c('locid','x','y')],by.x='first',by.y='locid',all.x=T)[,c('x','y','first')]
dist.frame[,c('x2','y2','id2')]<-merge(dist.frame[,'second',drop=F],df1[,c('locid','x','y')],by.x='second',by.y='locid',all.x=T)[,c('x','y','second')]
setwd("C:/Users/birde/Dropbox/refnet/git/refnet")
df<-data.frame("authorID"=c(1,2,3),
"AU"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"AF"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"groupID"= c(3,2,3),
"match_name"=c('Smith,J',NA,'Smith, Jon J'),
"similarity"= c(0.8833333,NA,0.8833333),
"author_order"= c(1,2,1),
"address"=c("Univ Florida, Gainesville, FL USA",
"University of Texas, Austin, TX, USA",NA),
"RP_address"=c("Univ Florida, Gainesville, FL USA",
"University of Oxford, Oxfordshire, UK",
"University of California Berkley, Berkley, CA, USA"),
"RI"=NA,
"OI"=NA,
"EM"=c("j.smith@ufl.edu",NA,'jsmith@usgs.gov'),
"refID"=c(1,1,3),
"TA"=NA,
"SO"=NA,
"UT"=c('test1','test1','test2'),
"PT"=NA,
"PU"=NA,
"PY"=NA,
"university"=c('Univ Florida','University of Oxford',
"University of California"),
"country"=c('USA','United Kingdom',"USA"),
"state"=c('FL','Oxfordshire',"CA"),
"postal_code"=NA,
"city"=c('Gainesville','Oxford',"Berkley"),
"department"=NA ,
"lat"=c(35,51.7520,39),
"lon"=c(-100,1.2577,-80),
stringsAsFactors=FALSE )
a<-plot(lat,lon,data=df)
a<-plot(lat~lon,data=df)
vdiffr::expect_doppleganger('testplot',a)
manage_cases()
library(vdiffr)
vdiffr::expect_doppelganger('testplot',a)
manage_cases()
manage_cases()
manage_cases()
ggplot(df, aes(x=lon,y=lat)) +geom_points()
library(ggplot2)
ggplot(df, aes(x=lon,y=lat)) +geom_points()
ggplot(df, aes(x=lon,y=lat)) +geom_point()
a<-ggplot(df, aes(x=lon,y=lat)) +geom_point()
vdiffr::expect_doppelganger('testplot',a)
manage_cases()
a<-ggplot(df, aes(x=lon,y=lat)) +geom_point()
a$plot
vdiffr::expect_doppelganger('testplot',a)
vdiffr::expect_doppelganger('testplot',a)
a
a<-ggplot(df, aes(x=lon,y=lat)) +geom_point()
vdiffr::expect_doppelganger('testplot',a)
manage_cases9
manage_cases()
function (title, fig, path = NULL, ..., user_fonts = NULL, verbose = FALSE)
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
validate_cases()
validate_cases()
manage_cases()
validate_cases
validate_cases()
validate_cases('refnet')
validate_cases(refnet)
git status
require(ggplot2)
df<-data.frame("authorID"=c(1,2,3),
"AU"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"AF"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"groupID"= c(3,2,3),
"match_name"=c('Smith,J',NA,'Smith, Jon J'),
"similarity"= c(0.8833333,NA,0.8833333),
"author_order"= c(1,2,1),
"address"=c("Univ Florida, Gainesville, FL USA",
"University of Texas, Austin, TX, USA",NA),
"RP_address"=c("Univ Florida, Gainesville, FL USA",
"University of Oxford, Oxfordshire, UK",
"University of California Berkley, Berkley, CA, USA"),
"RI"=NA,
"OI"=NA,
"EM"=c("j.smith@ufl.edu",NA,'jsmith@usgs.gov'),
"refID"=c(1,1,3),
"TA"=NA,
"SO"=NA,
"UT"=c('test1','test1','test2'),
"PT"=NA,
"PU"=NA,
"PY"=NA,
"university"=c('Univ Florida','University of Oxford',
"University of California"),
"country"=c('USA','United Kingdom',"USA"),
"state"=c('FL','Oxfordshire',"CA"),
"postal_code"=NA,
"city"=c('Gainesville','Oxford',"Berkley"),
"department"=NA ,
"lat"=c(35,51.7520,39),
"lon"=c(-100,1.2577,-80),
stringsAsFactors=FALSE )
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
ggplot(df, aes(x=lon,y=lat)) +geom_point()
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
manage_cases()
a<-ggplot(df, aes(x=lon,y=lat)) +geom_point()
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
ggplot(df, aes(x=lon,y=lat)) +geom_point()
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
git status
require(ggplot2)
df<-data.frame("authorID"=c(1,2,3),
"AU"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"AF"= c('Smith, Jon J.','Thompson, Bob B.','Smith,J'),
"groupID"= c(3,2,3),
"match_name"=c('Smith,J',NA,'Smith, Jon J'),
"similarity"= c(0.8833333,NA,0.8833333),
"author_order"= c(1,2,1),
"address"=c("Univ Florida, Gainesville, FL USA",
"University of Texas, Austin, TX, USA",NA),
"RP_address"=c("Univ Florida, Gainesville, FL USA",
"University of Oxford, Oxfordshire, UK",
"University of California Berkley, Berkley, CA, USA"),
"RI"=NA,
"OI"=NA,
"EM"=c("j.smith@ufl.edu",NA,'jsmith@usgs.gov'),
"refID"=c(1,1,3),
"TA"=NA,
"SO"=NA,
"UT"=c('test1','test1','test2'),
"PT"=NA,
"PU"=NA,
"PY"=NA,
"university"=c('Univ Florida','University of Oxford',
"University of California"),
"country"=c('USA','United Kingdom',"USA"),
"state"=c('FL','Oxfordshire',"CA"),
"postal_code"=NA,
"city"=c('Gainesville','Oxford',"Berkley"),
"department"=NA ,
"lat"=c(35,51.7520,39),
"lon"=c(-100,1.2577,-80),
stringsAsFactors=FALSE )
a<-ggplot(df, aes(x=lon,y=lat)) +geom_point()
a
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
vdiffr::expect_doppelganger('testplot',a)
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point())
version_freetype()
git status
devtools::test()
collect_cases()
vdiffr::expect_doppelganger('testplot',ggplot(df, aes(x=lon,y=lat)) +geom_point(),path='refnet/tests/figs/',verbose=T)
vdiffr::expect_doppelganger('testplot',a)
vdiffr::expect_doppelganger('testplot',a,verbose=T)
vdiffr::expect_doppelganger('testplot',a,path='refnet/tests/figs/',verbose=T)
vdiffr::expect_doppelganger('testplot',a,path='test-plot-addresses',verbose=T)
vdiffr::expect_doppelganger('testplot',a,path='test-plot-addresses',verbose=T)
vdiffr::expect_doppelganger('testplot',a,path='/test-plot-addresses',verbose=T)
vdiffr::expect_doppelganger('testplot',a,verbose=T)
manage_cases()
manage_cases()
collect_cases()
devtools::test()
collect_cases()
vdiffr::expect_doppelganger('testplot',a)
vdiffr::expect_doppelganger('testplot',a,verbose=T)
collect_cases()
manage_cases()
collect_cases()
collect_cases()
manage_cases()
devtools::test()
manage_cases()
devtools::test()
collect_cases()
devtools::test()
devtools::test()
manage_cases()
